Logging: args  Namespace(lr=2e-05, batch_size=512, weight_decay=0.01, epochs=25, eval_epochs=1, use_model_preds=1, print_freq=500, max_positions_PE=50, max_items_QG=21, repo_name='philipphager/baidu-ultr_uva-mlm-ctr', perturbation_sampling=False, sampling_type='rand_perturb', ultr_models='ips', lr_drop=15, save_epochs=5, delta_retain=0.5, soft_labels=False, soft_base=0.9, soft_gain=0.02, lr_drop_epochs=None, clip_max_norm=0.1, n_gpus=2, problem_type='classification', save_fname=None, output_path='/home/ec2-user/workspace/outputs/', data_path='/home/ec2-user/workspace/data/custom_click', output_folder='rr_ips_soft_pre', load_path='', load_path_reward='/home/ec2-user/workspace/outputs/ultr_ips/checkpoints/checkpoint30.pth', load_path_ranker='/home/ec2-user/workspace/outputs/ranker_ips/checkpoints/checkpoint20.pth', device='cuda', seed=42, n_viz=0, resume=0, start_epoch=0, eval=False, ste=False, concat_feats=False, pretrain_ranker=True, merge_imgs=False, train_ranker=True, train_ranker_lambda=False, eval_rels=False, force_tnse=False, use_dcg=False, save_cls=False, debug=False, use_wandb=False, use_doc_feat=True, num_workers=4, log_file=<_io.TextIOWrapper name='/home/ec2-user/workspace/outputs/rr_ips_soft_pre/out.log' mode='a' encoding='UTF-8'>, wandb_project_name='ranking', limit_train_batches=None, limit_val_batches=None, limit_test_batches=None, output_dir='/home/ec2-user/workspace/outputs/rr_ips_soft_pre')

 Resuming  reward _model from :  /home/ec2-user/workspace/outputs/ultr_ips/checkpoints/checkpoint30.pth

 Resuming  arranger _model from :  /home/ec2-user/workspace/outputs/ranker_ips/checkpoints/checkpoint20.pth
Logging: args  Namespace(lr=2e-05, batch_size=512, weight_decay=0.01, epochs=25, eval_epochs=1, use_model_preds=1, print_freq=500, max_positions_PE=50, max_items_QG=21, repo_name='philipphager/baidu-ultr_uva-mlm-ctr', perturbation_sampling=False, sampling_type='rand_perturb', ultr_models='ips', lr_drop=15, save_epochs=5, delta_retain=0.5, soft_labels=False, soft_base=0.9, soft_gain=0.02, lr_drop_epochs=None, clip_max_norm=0.1, n_gpus=2, problem_type='classification', save_fname=None, output_path='/home/ec2-user/workspace/outputs/', data_path='/home/ec2-user/workspace/data/custom_click', output_folder='rr_ips_soft_pre', load_path='', load_path_reward='/home/ec2-user/workspace/outputs/ultr_ips/checkpoints/checkpoint30.pth', load_path_ranker='/home/ec2-user/workspace/outputs/ranker_ips/checkpoints/checkpoint20.pth', device='cuda', seed=42, n_viz=0, resume=0, start_epoch=0, eval=False, ste=False, concat_feats=False, pretrain_ranker=True, merge_imgs=False, train_ranker=True, train_ranker_lambda=False, eval_rels=False, force_tnse=False, use_dcg=False, save_cls=False, debug=False, use_wandb=False, use_doc_feat=True, num_workers=4, log_file=<_io.TextIOWrapper name='/home/ec2-user/workspace/outputs/rr_ips_soft_pre/out.log' mode='a' encoding='UTF-8'>, wandb_project_name='ranking', limit_train_batches=None, limit_val_batches=None, limit_test_batches=None, output_dir='/home/ec2-user/workspace/outputs/rr_ips_soft_pre')

 Resuming  reward _model from :  /home/ec2-user/workspace/outputs/ultr_ips/checkpoints/checkpoint30.pth

 Resuming  arranger _model from :  /home/ec2-user/workspace/outputs/ranker_ips/checkpoints/checkpoint20.pth

 Val acc after  0  epochs :  tensor(11.0126, device='cuda:0', dtype=torch.float64)   loss :  0.6324998522724347

 Train acc after  0  epochs :  74.73587273316062   loss :  0.6323468847895223

 Val acc after  1  epochs :  tensor(11.1188, device='cuda:0', dtype=torch.float64)   loss :  0.6335516547460737

 Train acc after  1  epochs :  74.38145419545192   loss :  0.6323506043441365

 Val acc after  2  epochs :  tensor(11.0756, device='cuda:0', dtype=torch.float64)   loss :  0.6333529274458509

 Train acc after  2  epochs :  74.19356289579736   loss :  0.6322528930966329

 Val acc after  3  epochs :  tensor(11.3849, device='cuda:0', dtype=torch.float64)   loss :  0.6353514402142632

 Train acc after  3  epochs :  73.01550356217616   loss :  0.6339908388451709

 Val acc after  4  epochs :  tensor(11.5783, device='cuda:0', dtype=torch.float64)   loss :  0.6378111241579634

 Train acc after  4  epochs :  72.80579843120323   loss :  0.6351675530891374

 Val acc after  5  epochs :  tensor(11.2987, device='cuda:0', dtype=torch.float64)   loss :  0.6345889454980674

 Saving at epoch  5

 Train acc after  5  epochs :  73.68824661773172   loss :  0.6330868056339279

 Val acc after  6  epochs :  tensor(11.4437, device='cuda:0', dtype=torch.float64)   loss :  0.6347005205077799

 Train acc after  6  epochs :  73.61122355354058   loss :  0.6330924508471404

 Val acc after  7  epochs :  tensor(11.5291, device='cuda:0', dtype=torch.float64)   loss :  0.6359686648788516

 Train acc after  7  epochs :  73.5014797423719   loss :  0.6331166437665181

 Val acc after  8  epochs :  tensor(11.7318, device='cuda:0', dtype=torch.float64)   loss :  0.638846587168335

 Train acc after  8  epochs :  73.41793501727116   loss :  0.633507565200844

 Val acc after  9  epochs :  tensor(11.8043, device='cuda:0', dtype=torch.float64)   loss :  0.6403715966195834

 Train acc after  9  epochs :  72.92375053972366   loss :  0.635873835504814

 Val acc after  10  epochs :  tensor(12.7958, device='cuda:0', dtype=torch.float64)   loss :  0.6469784633977905

 Saving at epoch  10

 Train acc after  10  epochs :  72.28305357656879   loss :  0.6362795450055578

 Val acc after  11  epochs :  tensor(11.4815, device='cuda:0', dtype=torch.float64)   loss :  0.6352368708535119

 Train acc after  11  epochs :  72.28147938255613   loss :  0.6357853133669203

 Val acc after  12  epochs :  tensor(13.2324, device='cuda:0', dtype=torch.float64)   loss :  0.6501805290162594

 Train acc after  12  epochs :  72.22784434369603   loss :  0.6349760526985002

 Val acc after  13  epochs :  tensor(13.6833, device='cuda:0', dtype=torch.float64)   loss :  0.6545272319842315

 Train acc after  13  epochs :  71.88759355210132   loss :  0.6367150816491582

 Val acc after  14  epochs :  tensor(13.1021, device='cuda:0', dtype=torch.float64)   loss :  0.6509599824977677

 Train acc after  14  epochs :  72.00037330886586   loss :  0.6361290772765861

 Val acc after  15  epochs :  tensor(12.7906, device='cuda:0', dtype=torch.float64)   loss :  0.6490177128596444

 Saving at epoch  15

 Train acc after  15  epochs :  72.22761945883707   loss :  0.6352775716581502

 Val acc after  16  epochs :  tensor(12.4810, device='cuda:0', dtype=torch.float64)   loss :  0.6463437630311282

 Train acc after  16  epochs :  72.270347582038   loss :  0.6349210960988068

 Val acc after  17  epochs :  tensor(12.5356, device='cuda:0', dtype=torch.float64)   loss :  0.646130587676887

 Train acc after  17  epochs :  72.30576694732297   loss :  0.6349064211407521

 Val acc after  18  epochs :  tensor(12.6464, device='cuda:0', dtype=torch.float64)   loss :  0.6465159362418219

 Train acc after  18  epochs :  72.27147200633276   loss :  0.634455729667091

 Val acc after  19  epochs :  tensor(11.9157, device='cuda:0', dtype=torch.float64)   loss :  0.6390883990432843

 Train acc after  19  epochs :  72.42450615284974   loss :  0.6343574680339643

 Val acc after  20  epochs :  tensor(11.3813, device='cuda:0', dtype=torch.float64)   loss :  0.634223443830814

 Saving at epoch  20

 Train acc after  20  epochs :  72.55111632843983   loss :  0.6332143514746602

 Val acc after  21  epochs :  tensor(11.3956, device='cuda:0', dtype=torch.float64)   loss :  0.6346269875964897

 Train acc after  21  epochs :  72.91779109096143   loss :  0.6317432671150592

 Val acc after  22  epochs :  tensor(11.3889, device='cuda:0', dtype=torch.float64)   loss :  0.6347634062280185

 Train acc after  22  epochs :  72.9325210492228   loss :  0.6316351219285182

 Val acc after  23  epochs :  tensor(11.3919, device='cuda:0', dtype=torch.float64)   loss :  0.6347735568846612

 Train acc after  23  epochs :  72.82457631692573   loss :  0.6316694661886246

 Val acc after  24  epochs :  tensor(11.3067, device='cuda:0', dtype=torch.float64)   loss :  0.6340127487082728

 Train acc after  24  epochs :  72.97232566925734   loss :  0.6314971958995705
